---
title: "Scan Quality Assessor: Model Comparison"
author: "Heike Hofmann, Craig Orman, Naga Vempati"
output: html_document
format:
  html:
    toc: true
    toc-location: right
---
```{r, include = FALSE, warning=FALSE}
if (!require(tidyverse)) install.packages('tidyverse')
if (!require(ggplot2)) install.packages('ggplot2')
if (!require(randomForest)) install.packages('randomForest')
if (!require(irr)) install.packages('irr')
if (!require(corrplot)) install.packages('corrplot')
if (!require(MASS)) install.packages('MASS')
if (!require(RColorBrewer)) install.packages('RColorBrewer')
if (!require(yardstick)) install.packages('yardstick')
if (!require(caret)) install.packages('caret')
if (!require(nnet)) install.packages('nnet')
library(tidyverse)
library(ggplot2)
library(randomForest)
library(irr)
library(corrplot)
library(MASS)
library(RColorBrewer)
library(yardstick)
library(caret)
library(nnet)
set.seed(10247693)
## Load Functions - eventually we would like to just load a library
theme_set(theme_bw())
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

source("../R/comparison.R")

colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}

hh <- function(x) {
  colorize(x, "darkorange")
}

nv <- function(x) {
  colorize(x, "chartreuse3")
}

co <- function(x) {
  colorize(x, "red")
}
```

## Introduction
This document will give well commented exact technical details to show the comparison of various models we tested and compared to determine the best candidate for use in a production environment.

We will investigate the ability of our features to predict the quality of a scan using a variety of methods. It should be noted that given our current classification system Good > Tiny Problems > Problematic > Bad > Yikes, we will consider that any scan worse than Tiny Problems should be re-scanned by the user. However, this is a tool meant to aid a user in deciding and is not to be taken as a sole source of truth. 

Models will be broken into two classification types, ordinal and binary. In the ordinal models, we will try to accurately predict which of the five categories a scan falls into. As quality is a subjective rating, we will focus more on being within one step of the assigned rating than going for perfect agree, although perfect agreement would be optimal. In the binary setting, we will group the scans as Good (1) and Bad (0) as binary classifications are often easier. Good (1) scans will be scans currently labelled as Good or Tiny Problems, with Bad (0) scans being Problematic, Bad, and Yikes. 

- `r co("Is there a more mathy and rigorous reason we chose these models? They are pretty much just my go-tos, and the only ones I know for categorical")` 

XXX results first introduction to save people time


The models we will train are logistic regressions, and random forests. These models have often proven to be sufficiently explainable for forensic purposes, as well as statistically sound in many environments. 

Data handling and type setting

```{r}
full.data <- read.csv2("../data/std_and_cropped_data_12_20_2022.csv", sep=",")
full.data <- full.data %>% mutate(
  Quality = factor(Quality, levels = c("Good", "Tiny Problems", "Problematic", "Bad", "Yikes"), ordered = TRUE),
  Problem = factor(Problem, levels = c("Good", "Damage", "Holes", "Feathering", "Rotation-Staging"), ordered = FALSE),
  GoodScan = Quality %in% c("Good", "Tiny Problems") %>% factor(),
  LAPD_id = sprintf("FAU%3d-B%s-L%d",FAU, Bullet, Land),
  
  # This is the features ran against the full image
  assess_percentile_na_proportion = as.numeric(assess_percentile_na_proportion),
  assess_col_na = as.numeric(assess_col_na),
  extract_na = as.numeric(extract_na),
  assess_middle_na_proportion = as.numeric(assess_middle_na_proportion),
  assess_rotation = as.numeric(assess_rotation),
  assess_bottomempty = as.numeric(assess_bottomempty),
  assess_median_na_proportion = as.numeric(assess_median_na_proportion),
  
  # This is the features ran against the cropped image
  assess_percentile_na_proportion_cropped = as.numeric(assess_percentile_na_proportion_cropped),
  assess_col_na_cropped = as.numeric(assess_col_na_cropped),
  extract_na_cropped = as.numeric(extract_na_cropped),
  assess_bottomempty_cropped = as.numeric(assess_bottomempty_cropped),
  assess_median_na_proportion_cropped = as.numeric(assess_median_na_proportion_cropped)
)


followupScans <- data.frame()
```

## Feature Analysis

In this investigation, we are going to look at the features calculated against the full image. This investigation will tell us more about the predictive power of each feature, and tell us how different the quality categories are from each other.

XXX Upfront table of the results

|   |Ord Log|Num Log|Ord For|Num For|
|---|-------|-------|-------|-------|
|TPR|  |  |  |  |
|TNR|  |  |  |  |

XXX Our results are mostly in graphical form. Should I use some kinda test?

```{r}
standard.data <- full.data[,1:12 & 18:19]
knitr::kable(table(standard.data$Quality, standard.data$Problem),
             caption = "Full Data Problem by Quality")
```

### Assess Percentile NA Proportion 

```{r}
summary(standard.data$assess_percentile_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_percentile_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Percentile NA Proportion by Quality")

summary(glm(Quality ~ assess_percentile_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Percentile NA Proportion increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence, with reservation, for this being a useful feature in explaining the quality.

### Assess Col NA

```{r}
summary(standard.data$assess_col_na)
ggplot(standard.data, aes(x=Quality, y=assess_col_na)) +
  geom_boxplot() +
  ggtitle("Assess Col NA by Quality")
summary(glm(Quality ~ assess_col_na,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Col NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart, with the exception of "Good" and "Tiny Problems" which are very similar. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Extract NA

```{r}
summary(standard.data$extract_na)
ggplot(standard.data, aes(x=Quality, y=extract_na)) +
  geom_boxplot() +
  ggtitle("Extract NA by Quality")
summary(glm(Quality ~ extract_na,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Extract NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Middle NA Proportion

```{r}
summary(standard.data$assess_middle_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_middle_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Middle NA Proportion by Quality")
summary(glm(Quality ~ assess_middle_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Extract NA increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Rotation

```{r}
summary(standard.data$assess_rotation)
ggplot(standard.data, aes(x=Quality, y=assess_rotation)) +
  geom_boxplot() +
  ggtitle("Assess Rotation by Quality")
summary(glm(Quality ~ assess_rotation,
                     data=standard.data, family="binomial"))
```

In the boxplot, we see that there is no significant visual difference in median or IQR of Assess Rotation when grouped by Quality. The logistic regression shows no evidence that this feature can help explain the quality of an image. This feature was primarily intended for use in predicting a particular problem that occurs in scans. 

### Assess Bottomempty

```{r}
summary(standard.data$assess_bottomempty)
ggplot(standard.data, aes(x=Quality, y=assess_bottomempty)) +
  geom_boxplot() +
  ggtitle("Assess Bottomempty by Quality")
summary(glm(Quality ~ assess_bottomempty,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Bottomempty increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. There is an observation of note that the Yikes category has a particularly large IQR. The logistic regression also shows that there is overwhelming evidence for this being a useful feature in explaining the quality.

### Assess Median NA Proportion

```{r}
summary(standard.data$assess_median_na_proportion)
ggplot(standard.data, aes(x=Quality, y=assess_median_na_proportion)) +
  geom_boxplot() +
  ggtitle("Assess Median NA Proportion by Quality")
summary(glm(Quality ~ assess_median_na_proportion,
                     data=standard.data, family="binomial"))
```

In the boxplot we can see that as Assess Median NA Proportion increases, the quality of the image decreases. The IQR of the quality categories has overlap, but the medians are consistently different in the chart. The "Bad" and "Yikes" categories have very similar IQR and Medians. The logistic regression also shows that there is overwhelming evidence, with reservation, for this being a useful feature in explaining the quality.

### Cross Correlations

XXX include https://heike.github.io/ggpcp/ and a scatterplot of the data
```{r}
correlations <- cor(standard.data[,6:12])
corrplot(correlations, method = "shade")
```

There is significant correlation between most of the variables except assess rotation. 

## Model Analysis

XXX Why did I not include all the features in the models?!?!?! GRRRRR
XXX REDO: all models with correct equations

| | Standard Multinomial Logistic Regression | Standard Numerical Logistic Regression | Standard Ordinal Random Forest| Standard Numerical Random Forest|
|---|-------|-------|-------|-------|
|F1 |  |  |  |  |
|TPR|  |  |  |  |
|TNR|  |  |  |  |

**Categorical Predictions:**

For the purposes of this analysis, consider that the scanner using this model to decide if a scan needs to be re-done will be instructed that Good and Tiny Problems predictions likely don't need to be re-done. However, Problematic, Bad, and Yikes scans have a high likelihood of needing to be redone.

**Quantitative Predictions:**

    --cutoff is....

```{r}
sample <- sample(c(TRUE, FALSE), nrow(standard.data), replace=TRUE, prob=c(0.75,0.25))
train  <- standard.data[sample, ]
test   <- standard.data[!sample, ]

train$followup <- FALSE
train$num.Quality <- 0
train$num.Quality[train$Quality == "Good"] <- 1
train$num.Quality[train$Quality == "Tiny Problems"] <- 1
train$num.Quality <- factor(train$num.Quality, levels = c(0, 1))

test$followup <- FALSE
test$num.Quality <- 0
test$num.Quality[test$Quality == "Good"] <- 1
test$num.Quality[test$Quality == "Tiny Problems"] <- 1
test$num.Quality <- factor(test$num.Quality, levels = c(0, 1))

knitr::kable(table(standard.data$Quality, standard.data$Problem),
             caption = "Full Data Problem by Quality")
knitr::kable(table(train$Quality, train$Problem), caption = "Train Problem by Quality")
knitr::kable(table(test$Quality, test$Problem), caption = "Test Problem by Quality")

knitr::kable(table(train$num.Quality), caption = "Train Numeric Quality Totals")
knitr::kable(table(train$num.Quality, train$Problem), caption = "Train Numeric Problem by Quality")

knitr::kable(table(test$num.Quality), caption = "Test Numeric Quality Totals")
knitr::kable(table(test$num.Quality, test$Problem), caption = "Test Numeric Problem by Quality")
```


### Standard Multi-Nomial Logistic Regression

XXX MultiNomial Model!

The assumptions of the multinomial logisitic regression are:
  - Nominal dependent variable: Not met
  - No multi-colinearity: 
  - Linear relationship:
  - No outlier or high influence points: 
  
```{r, eval = FALSE}
train.mutli = train
train.mutli$Quality <- factor(train.mutli$Quality, levels=c("Good", "Tiny Problems", "Problematic", "Bad", "Yikes"), ordered = FALSE)
train.mutli$Quality <- relevel(train.mutli$Quality, ref = "Good")
multi.model <- multinom(Quality ~ assess_percentile_na_proportion + assess_col_na +
                         extract_na + assess_middle_na_proportion + assess_bottomempty +
                         assess_median_na_proportion, data = train.mutli)
summary(multi.model)

test$Predictions <- predict(multi.model, test)
```

To help compare these models, we will introduce a compression concept. Just as before, all scans will be divided into Good, which includes Good and Tiny Problems. Bad will include all other categories.
```{r}
test$num.predictions = 0
test$num.predictions[test$Predictions == "Good"] = 1
test$num.predictions[test$Predictions == "Tiny Problems"] = 1
test$num.predictions <- factor(test$num.predictions, levels = c(0, 1))

# Top is true value, left is predicted
metrics <- confusionMatrix(test$num.predictions,
                           test$num.Quality, mode="everything", positive = "1")
metrics
```

### Standard Numerical regression
  - (Good/Tiny = 1, Problematic/Bad/Yikes = 0)

```{r}
num.logit.model <- glm(num.Quality ~ assess_percentile_na_proportion + assess_col_na +
                         extract_na + assess_middle_na_proportion + assess_bottomempty +
                         assess_median_na_proportion,
                     data=train, family="binomial")

summary(num.logit.model)

test$num.predictions <- predict(num.logit.model, test, type="response")

# XXX Figure out followup scans
test[test$num.predictions < 0.5 & test$num.Quality == 1, ]$followup = TRUE
test[test$num.predictions > 0.75 & test$num.Quality == 0, ]$followup = TRUE

ggplot(test, aes(x=num.Quality, y=num.predictions)) +
  geom_boxplot() +
  xlab("True Quality") +
  ylab("Probability of good scan") +
  ggtitle("Numerical Logistic Prediction Test Data")

ggplot(test, aes(x=num.Quality, y=num.predictions, color=followup)) +
  geom_jitter() +
   scale_colour_manual(values=c("grey50", "darkorange"))

```

The boxplot shows a general picture of the results of the model. There is a significant number of misclassified scans. Using a typical 0.5 rounding we get the confusion matrix: 
```{r}
# Top is true value, left is predicted
metrics <- confusionMatrix(factor(round(test$num.predictions)),
                           test$num.Quality, mode="everything", positive = "1")
metrics
```

### Standard Ordinal random forest

Metrics 
    -McNemars
    -kappas
    -various agreement outputs and stuff.

```{r}
ord.forest.model <- randomForest(Quality ~ assess_percentile_na_proportion + assess_col_na +
                         extract_na + assess_middle_na_proportion + assess_bottomempty +
                         assess_median_na_proportion, data = train,
                           importance = TRUE)

test$Prediction <- predict(ord.forest.model, test)
```

To help compare these models, we once again use the previous compression metrics.
```{r}
test$num.predictions = 0
test$num.predictions[test$Predictions == "Good"] = 1
test$num.predictions[test$Predictions == "Tiny Problems"] = 1
test$num.predictions <- factor(test$num.predictions, levels = c(0, 1))


# Top is true value, left is predicted
metrics <- confusionMatrix(test$num.predictions,
                           test$num.Quality, mode="everything", positive = "1")

metrics
```

### Standard Numerical Random Forest
  - (Good/Tiny = 1, Problematic/Bad/Yikes = 0)
  - (Good = 1, Yikes = 0, drop others)

```{r}
num.RF.model <- randomForest(num.Quality ~ assess_percentile_na_proportion + assess_col_na +
                         extract_na + assess_middle_na_proportion + assess_bottomempty +
                         assess_median_na_proportion, data = train,
                           importance = TRUE)

test$num.predictions <- predict(num.RF.model, test, type="prob")[,2]

test[test$num.predictions < 0.5 & test$num.Quality == 1, ]$followup = TRUE
test[test$num.predictions > 0.75 & test$num.Quality == 0, ]$followup = TRUE

ggplot(test, aes(x=num.Quality, y=num.predictions)) +
  geom_boxplot() +
  xlab("True Quality") +
  ylab("Probability of good scan") +
  ggtitle("Numerical Logistic Prediction Test Data")

ggplot(test, aes(x=num.Quality, y=num.predictions, color=followup)) +
  geom_jitter() +
   scale_colour_manual(values=c("grey50", "darkorange"))

```

The boxplot shows a general picture of the results of the model. There is a significant number of misclassified scans. Using a typical 0.5 rounding we get the confusion matrix: 
```{r}
# Top is true value, left is predicted
metrics <- confusionMatrix(factor(round(test$num.predictions)),
                           test$num.Quality, mode="everything", positive = "1")
metrics
```

## Cropped Feature Analysis

#### Hypothesis
The quality of our images predicted by the models and assessed by our algorithms is dependent on the how much of the data from the scan is legible and useful. The bottom middle of the bullet gives us the most information, due to the striations in this area being most prominent. Some scans get distorted due to numerous external factors ranging from a bad scan to the bullet undergoing damages that's affected those striations. Cropping our scans to cut off parts of the scan that are non-essential could help to eliminate the ‘noise’ around the images that skew the accuracy of the results. The goal is to see whether making a crop will improve the accuracy of our models. 


#### Process
To conduct our analysis, we did a series of tests to compare the cropped version of a scan against the full version. We constructed scatter and box plots as well as ROC curves for visual analysis. We trained a Generalized Linear Model (glm) to test the p values, or importance, of each variable in explaining the quality, and constructed kernel density graphs to compare the scans. Every cropped scan and its full scan counterpart were found to have more than 90% correlation with each other, requiring us to use only one of them for each feature to avoid co-linearity issues.

```{r, echo = FALSE}
feature_table <- data.frame(matrix(nrow = 4, ncol=5, dimnames=list(c("Extract NA", "Assess Bottomempty", "Assess Col NA", "Assess Median NA Proportion"), c("Correlation", "pvalue_Full", "pvalue_cropped", "auc_Full", "auc_Cropped"))))

feature_table[1,] <- c(0.908, "0.229", "<2e-16", 0.871, 0.902)
feature_table[2,] <- c(0.905, "5.86e-10", "<2e-16", 0.783, 0.859)
feature_table[3,] <- c(0.919, "1.62e-06", "1.17e-14", 0.888, 0.896)
feature_table[4,] <- c(0.908, "<2e-16", "6.77e-10", 0.907, 0.863)

knitr::kable(feature_table)
```

### Extract NA Cropped

#### Which of the features is better for discriminating between good and bad scans?

```{r echo=FALSE}
correlation <- cor(full.data$extract_na, full.data$extract_na_cropped)

res <- comparison(data.frame(full.data$extract_na, full.data$extract_na_cropped, full.data$Quality), feature = "Extract NA")

res$scatterplot + coord_equal()

res$boxplot

res$roc_curve

print(paste("Extract NA. Correlation: ", round(correlation, 3), "Full AUC:", round(res$roc_auc$Full_AUC, 3), "Cropped AUC: ", round(res$roc_auc$Cropped_AUC, 3)))

knitr::kable(res$summ, caption=attr(res$summ, "title"))

```
#### Should we use features from just one type of scan or both?

```{r, echo=FALSE}
# logistic regression in the two features
logistic_base <- glm(GoodScan~extract_na+extract_na_cropped, data = full.data, family = binomial())
summary(logistic_base)

# extract_na_cropped is the better single predictor. 
full.data %>% pivot_longer(starts_with("extract_na"), names_to="Scan") %>% 
  ggplot(aes(x = value, fill=GoodScan, color=GoodScan)) +
  geom_density(alpha=0.8) +
  scale_fill_manual(values=col_scans_light) +
  scale_colour_manual(values=col_scans_dark) +
  facet_grid(.~Scan)

```


#### Conclusion for Extract NA

The values for feature `extract_NA` are highly correlated between the cropped and the full scan. 

Using good and scans with only tiny problems as overall 'good' scans, the feature applied to cropped scans has an increased accuracy compared to the feature values from the full scan. 

We might want to follow up on the orange colored scans:

```{r echo=FALSE, fig.height=3}
full.data  <- full.data %>% 
  mutate(followup=GoodScan=="TRUE" & extract_na_cropped>15)
full.data %>% 
  ggplot(aes(x = extract_na_cropped, y = GoodScan, color = followup)) + 
  geom_jitter() +
  scale_colour_manual(values=c("grey50", "darkorange"))


```
```{r}

full.data$LAPD_id[full.data$followup]

followupScans <- rbind(followupScans, full.data[full.data$followup == TRUE,])
# All followups for extract_na are mislabelled scans. They are all labelled as tiny problems but should be problematic or worse.
```

```{r, include = FALSE, eval = FALSE}
library(x3ptools)
# /media/Raven/LAPD
f1 <- x3p_read("/media/Raven/LAPD/FAU 263/Bullet A/LAPD - 263 - Bullet A - Land 4 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU263-BA-L4 is labelled tiny-problems but should be labelled Problematic or worse

f2 <- x3p_read("/media/Raven/LAPD/FAU 263/Bullet C/LAPD - 263 - Bullet C - Land 1 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU263-BC-L1 is labelled tiny-problems but should be labelled Problematic or worse

f3 <- x3p_read("/media/Raven/LAPD/FAU 263/Bullet C/LAPD - 263 - Bullet C - Land 3 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU263-BC-L3 is labelled tiny-problems but should be labelled Problematic or worse

f4 <- x3p_read("/media/Raven/LAPD/FAU 287/Bullet C/LAPD - 287 - Bullet C - Land 5 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU287-BC-L5 is labelled tiny-problems but should be labelled Problematic or worse

f5 <- x3p_read("/media/Raven/LAPD/FAU 154/Bullet D/LAPD - 154 - Bullet D - Land 2 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Carley McConnell.x3p")
# FAU154-BD-L2 is labelled tiny-problems but should be labelled Problematic or worse

f6 <- x3p_read("/media/Raven/LAPD/FAU 277/Bullet A/LAPD - 277 - Bullet A - Land 4 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU277-BA-L4 is labelled tiny-problems but should be labelled Problematic or worse

f7 <- x3p_read("/media/Raven/LAPD/FAU 286/Bullet A/LAPD - 286 - Bullet A - Land 5 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
# FAU286-BA-L5 is labelled tiny-problems but should be labelled Problematic or worse
```


### Assess Bottomempty Cropped

#### Which of the features is better for discriminating between good and bad scans?

```{r echo=FALSE}
correlation <- cor(full.data$assess_bottomempty, full.data$assess_bottomempty_cropped)

res <- comparison(data.frame(full.data$assess_bottomempty, full.data$assess_bottomempty_cropped, full.data$Quality), feature = "Assess Bottomempty")
res$scatterplot + coord_equal()

res$boxplot

res$roc_curve

print(paste("Assess Bottomempty. Correlation: ", round(correlation, 3), "Full AUC:", round(res$roc_auc$Full_AUC, 3), "Cropped AUC: ", round(res$roc_auc$Cropped_AUC, 3)))

knitr::kable(res$summ, caption=attr(res$summ, "title"))

```
#### Should we use features from just one type of scan or both?

```{r, echo=FALSE}
# logistic regression in the two features
logistic_base <- glm(GoodScan~assess_bottomempty+assess_bottomempty_cropped, data = full.data, family = binomial())
summary(logistic_base)

# assess_bottomempty_cropped is the better single predictor. 
full.data %>% pivot_longer(starts_with("assess_bottomempty"), names_to="Scan") %>% 
  ggplot(aes(x = value, fill=GoodScan, color=GoodScan)) + geom_density(alpha=0.8) + scale_fill_manual(values=col_scans_light) + scale_colour_manual(values=col_scans_dark) +
  facet_grid(.~Scan)
```


#### Conclusion for Assess Bottomempty

The values for feature `assess_bottomempty` are highly correlated between the cropped and the full scan. 

Using good and scans with only tiny problems as overall 'good' scans, the feature applied to cropped scans has an increased accuracy compared to the feature values from the full scan. 

We might want to follow up on the orange colored scans:

```{r echo=FALSE, fig.height=3}
full.data  <- full.data %>% 
  mutate(followup=GoodScan=="TRUE" & assess_bottomempty_cropped>30)
full.data %>% 
  ggplot(aes(x = assess_bottomempty_cropped, y = GoodScan, color = followup)) + 
  geom_jitter() +
  scale_colour_manual(values=c("grey50", "darkorange"))
```
```{r}
full.data$LAPD_id[full.data$followup]

followupScans <- rbind(followupScans, full.data[full.data$followup == TRUE,])
```

```{r, include = FALSE, eval = FALSE}
# All problems are registered as tiny problems
followup <- c("FAU263-BA-L4", "FAU287-BC-L5", "FAU254-BD-L4", "FAU275-BC-L5", "FAU275-BD-L3", "FAU277-BA-L4", "FAU286-BA-L5")
f1 <- x3p_read("/media/Raven/LAPD/FAU 263/Bullet A/LAPD - 263 - Bullet A - Land 4 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f1, main=followup[1])

f2 <- x3p_read("/media/Raven/LAPD/FAU 287/Bullet C/LAPD - 287 - Bullet C - Land 5 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f2, main=followup[2])

f3 <- x3p_read("/media/Raven/LAPD/FAU 254/Bullet D/LAPD - 254 - Bullet D - Land 4 - Sneox2 - 20x - auto light left image +20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f3, main=followup[3])

f4 <- x3p_read("/media/Raven/LAPD/FAU 275/Bullet C/LAPD - 275 - Bullet C - Land 5 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f4, main=followup[4])

f5 <- x3p_read("/media/Raven/LAPD/FAU 275/Bullet D/LAPD - 275 - Bullet D - Land 3 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f5, main=followup[5])

f6 <- x3p_read("/media/Raven/LAPD/FAU 277/Bullet A/LAPD - 277 - Bullet A - Land 4 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f6, main=followup[6])

f7 <- x3p_read("/media/Raven/LAPD/FAU 286/Bullet A/LAPD - 286 - Bullet A - Land 5 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p")
image(f7, main=followup[7])

```


### Assess Col NA Cropped

#### Which of the features is better for discriminating between good and bad scans?

```{r echo=FALSE}
correlation <- cor(full.data$assess_col_na, full.data$assess_col_na_cropped)

res <- comparison(data.frame(full.data$assess_col_na, full.data$assess_col_na_cropped, full.data$Quality), feature = "Assess Col NA")

res$scatterplot + coord_equal()

res$boxplot

res$roc_curve

print(paste("Assess Col NA Correlation: ", round(correlation, 3), "Full AUC:", round(res$roc_auc$Full_AUC, 3), "Cropped AUC: ", round(res$roc_auc$Cropped_AUC, 3)))

knitr::kable(res$summ, caption=attr(res$summ, "title"))

```
#### Should we use features from just one type of scan or both?

```{r, echo=FALSE}
# logistic regression in the two features
logistic_base <- glm(GoodScan~assess_col_na+assess_col_na_cropped, data = full.data, family = binomial())
summary(logistic_base)

# Both predictors are about the same.
full.data %>% pivot_longer(starts_with("assess_col_na"), names_to="Scan") %>% 
  ggplot(aes(x = value, fill=GoodScan, color=GoodScan)) + geom_density(alpha=0.8) + scale_fill_manual(values=col_scans_light) + scale_colour_manual(values=col_scans_dark) +
  facet_grid(.~Scan)
```


#### Conclusion for Assess Col NA

The values for feature `assess_col_na` are highly correlated between the cropped and the full scan. 

Using good and scans with only tiny problems as overall 'good' scans, the feature applied to cropped scans has an increased accuracy compared to the feature values from the full scan. 

We might want to follow up on the orange colored scans:

```{r echo=FALSE, fig.height=3}
full.data  <- full.data %>% 
  mutate(followup=GoodScan=="TRUE" & assess_col_na_cropped>1.35)
full.data %>% 
  ggplot(aes(x = assess_col_na_cropped, y = GoodScan, color = followup)) + 
  geom_jitter() +
  scale_colour_manual(values=c("grey50", "darkorange"))
```
```{r}
full.data$LAPD_id[full.data$followup]

followupScans <- rbind(followupScans, full.data[full.data$followup == TRUE,])
```

```{r, include = FALSE, eval = FALSE}
followup <- data.frame(LAPD_ID = c("FAU263-BA-L4", "FAU263-BB-L3", "FAU263-BC-L1", "FAU263-BC-L3", "FAU154-BD-L2", "FAU286-BA-L5"), filePAth = c("/media/Raven/LAPD/FAU 263/Bullet A/LAPD - 263 - Bullet A - Land 4 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p", "/media/Raven/LAPD/FAU 263/Bullet B/LAPD - 263 - Bullet B - Land 3 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p", "/media/Raven/LAPD/FAU 263/Bullet C/LAPD - 263 - Bullet C - Land 1 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p", "/media/Raven/LAPD/FAU 263/Bullet C/LAPD - 263 - Bullet C - Land 3 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p", "/media/Raven/LAPD/FAU 154/Bullet D/LAPD - 154 - Bullet D - Land 2 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Carley McConnell.x3p"  , "/media/Raven/LAPD/FAU 286/Bullet A/LAPD - 286 - Bullet A - Land 5 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p"))

for (i in 1:nrow(followup)) {
  f <- x3p_read(followup[i,2])
  image(f, main=followup[i,1])
}
```

### Assess Median NA Proportion Cropped

#### Which of the features is better for discriminating between good and bad scans?

```{r echo=FALSE}
correlation <- cor(full.data$extract_na, full.data$extract_na_cropped)


res <- comparison(data.frame(full.data$assess_median_na_proportion, full.data$assess_median_na_proportion_cropped, full.data$Quality), feature = "Assess median NA proportion")

res$scatterplot + coord_equal()

res$boxplot

res$roc_curve

print(paste("Assess Median NA Proportion. Correlation: ", round(correlation, 3), "Full AUC:", round(res$roc_auc$Full_AUC, 3), "Cropped AUC: ", round(res$roc_auc$Cropped_AUC, 3)))

knitr::kable(res$summ, caption=attr(res$summ, "title"))

```
#### Should we use features from just one type of scan or both?

```{r, echo=FALSE}
# logistic regression in the two features
logistic_base <- glm(GoodScan~assess_median_na_proportion+assess_median_na_proportion_cropped,
                     data = full.data, family = binomial())
summary(logistic_base)

# assess_median_na_proportion is the better single predictor. 
full.data %>% pivot_longer(starts_with("assess_median_na_proportion"), names_to="Scan") %>% 
  ggplot(aes(x = value, fill=GoodScan, color=GoodScan)) + geom_density(alpha=0.8) + scale_fill_manual(values=col_scans_light) + scale_colour_manual(values=col_scans_dark) +
  facet_grid(.~Scan)
```


#### Conclusion for Assess Median NA Proportion

The values for feature `extract_NA` are highly correlated between the cropped and the full scan. 

Using good and scans with only tiny problems as overall 'good' scans, the feature applied to full scans has an increased accuracy compared to the feature values from the cropped scan. 

We might want to follow up on the orange colored scans:

```{r echo=FALSE, fig.height=3}
full.data  <- full.data %>% 
  mutate(followup=GoodScan=="TRUE" & assess_median_na_proportion>0.095)
full.data %>% 
  ggplot(aes(x = assess_median_na_proportion, y = GoodScan, color = followup)) + 
  geom_jitter() +
  scale_colour_manual(values=c("grey50", "darkorange"))
```
```{r}
full.data$LAPD_id[full.data$followup]

followupScans <- rbind(followupScans, full.data[full.data$followup == TRUE,])
```
```{r, include = FALSE, eval = FALSE}
followup <- data.frame(LAPD_ID = c("FAU263-BC-L3", "FAU154-BD-L2", "FAU204-BC-L4"), filePAth = c("/media/Raven/LAPD/FAU 263/Bullet C/LAPD - 263 - Bullet C - Land 3 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p", "/media/Raven/LAPD/FAU 154/Bullet D/LAPD - 154 - Bullet D - Land 2 - Sneox2 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Carley McConnell.x3p", "/media/Raven/LAPD/FAU 204/Bullet C/LAPD - 204 - Bullet C - Land 4 - Sneox1 - 20x - auto light left image + 20% - threshold 2 - resolution 4 - Connor Hergenreter.x3p"))

for (i in 1:nrow(followup)) {
  f <- x3p_read(followup[i,2])
  image(f, main=followup[i,1])
}
# all should be relabbeled, FAU 204, BC, L4 is particularly Yikes looking
```

## Cropped Model Analysis

| | Cropped Multinomial Logistic Regression | Cropped Numerical Logistic Regression | Cropped Ordinal Cropped Forest| Cropped Numerical Random Forest|
|---|-------|-------|-------|-------|
|F1 |  |  |  |  |
|TPR|  |  |  |  |
|TNR|  |  |  |  |


```{r}
sample <- sample(c(TRUE, FALSE), nrow(full.data), replace=TRUE, prob=c(0.75,0.25))
train  <- full.data[sample, ]
test   <- full.data[!sample, ]

train$followup <- FALSE
train$num.Quality <- 0
train$num.Quality[train$Quality == "Good"] <- 1
train$num.Quality[train$Quality == "Tiny Problems"] <- 1
train$num.Quality <- factor(train$num.Quality, levels = c(0, 1))

test$followup <- FALSE
test$num.Quality <- 0
test$num.Quality[test$Quality == "Good"] <- 1
test$num.Quality[test$Quality == "Tiny Problems"] <- 1
test$num.Quality <- factor(test$num.Quality, levels = c(0, 1))

knitr::kable(table(standard.data$Quality, standard.data$Problem),
             caption = "Full Data Problem by Quality")
knitr::kable(table(train$Quality, train$Problem), caption = "Train Problem by Quality")
knitr::kable(table(test$Quality, test$Problem), caption = "Test Problem by Quality")

knitr::kable(table(train$num.Quality), caption = "Train Numeric Quality Totals")
knitr::kable(table(train$num.Quality, train$Problem), caption = "Train Numeric Problem by Quality")

knitr::kable(table(test$num.Quality), caption = "Test Numeric Quality Totals")
knitr::kable(table(test$num.Quality, test$Problem), caption = "Test Numeric Problem by Quality")
```

XXX Upfront table of the results

### Cropped Multinomial Logistic Regression
```{r, eval = FALSE}
train.mutli = train
train.mutli$Quality <- factor(train.mutli$Quality, levels=c("Good", "Tiny Problems", "Problematic", "Bad", "Yikes"), ordered = FALSE)
train.mutli$Quality <- relevel(train.mutli$Quality, ref = "Good")
crop.multi.model <- multinom(Quality ~ assess_percentile_na_proportion + assess_col_na_cropped +
                         extract_na_cropped + assess_middle_na_proportion + assess_bottomempty_cropped +
                         assess_median_na_proportion, data = train.mutli)
summary(multi.model)

test$Predictions <- predict(multi.model, test)
```

To help compare these models, we will introduce a compression concept. Just as before, all scans will be divided into Good, which includes Good and Tiny Problems. Bad will include all other categories.
```{r}
test$num.predictions = 0
test$num.predictions[test$Predictions == "Good"] = 1
test$num.predictions[test$Predictions == "Tiny Problems"] = 1
test$num.predictions <- factor(test$num.predictions, levels = c(0, 1))

# Top is true value, left is predicted
metrics <- confusionMatrix(test$num.predictions,
                           test$num.Quality, mode="everything", positive = "1")
metrics

```

### Cropped Numerical Logistic Regression

```{r}
crop.num.logit.model <- glm(num.Quality ~ assess_percentile_na_proportion + assess_col_na_cropped +
                         extract_na_cropped + assess_middle_na_proportion + assess_bottomempty_cropped +
                         assess_median_na_proportion,
                     data=train, family="binomial")

summary(crop.num.logit.model)

test$num.predictions <- predict(crop.num.logit.model, test, type="response")

# XXX Figure out followup scans
test[test$num.predictions < 0.5 & test$num.Quality == 1, ]$followup = TRUE
test[test$num.predictions > 0.75 & test$num.Quality == 0, ]$followup = TRUE

ggplot(test, aes(x=num.Quality, y=num.predictions)) +
  geom_boxplot() +
  xlab("True Quality") +
  ylab("Probability of good scan") +
  ggtitle("Cropped Numerical Logistic Prediction Test Data")

ggplot(test, aes(x=num.Quality, y=num.predictions, color=followup)) +
  geom_jitter() +
   scale_colour_manual(values=c("grey50", "darkorange"))

```


The boxplot shows a general picture of the results of the model. There is a significant number of misclassified scans. Using a typical 0.5 rounding we get the confusion matrix: 
```{r}
# Top is true value, left is predicted
metrics <- confusionMatrix(factor(round(test$num.predictions)),
                           test$num.Quality, mode="everything", positive = "1")
metrics
```

### Cropped Ordinal Random Forest
```{r}
crop.ord.forest.model <- randomForest(Quality ~ assess_percentile_na_proportion +
                                      assess_col_na_cropped + extract_na_cropped +
                                      assess_middle_na_proportion + assess_bottomempty_cropped +
                                      assess_median_na_proportion, data = train,
                                      importance = TRUE)

test$Prediction <- predict(crop.ord.forest.model, test)
```

To help compare these models, we will introduce a compression concept. Just as before, all scans will be divided into Good, which includes Good and Tiny Problems. Bad will include all other categories.
```{r}
test$num.predictions = 0
test$num.predictions[test$Predictions == "Good"] = 1
test$num.predictions[test$Predictions == "Tiny Problems"] = 1
test$num.predictions <- factor(test$num.predictions, levels = c(0, 1))

# Top is true value, left is predicted
metrics <- confusionMatrix(test$num.predictions,
                           test$num.Quality, mode="everything", positive = "1")
metrics

```

### Cropped Numerical Random Forest
```{r}
crop.num.RF.model <- randomForest(num.Quality ~ assess_percentile_na_proportion + assess_col_na_cropped +
                         extract_na_cropped + assess_middle_na_proportion + assess_bottomempty_cropped +
                         assess_median_na_proportion, data = train,
                           importance = TRUE)


test$num.predictions <- predict(crop.num.RF.model, test, type="prob")[,2]

test[test$num.predictions < 0.5 & test$num.Quality == 1, ]$followup = TRUE
test[test$num.predictions > 0.75 & test$num.Quality == 0, ]$followup = TRUE

ggplot(test, aes(x=num.Quality, y=num.predictions)) +
  geom_boxplot() +
  xlab("True Quality") +
  ylab("Probability of good scan") +
  ggtitle("Numerical Logistic Prediction Test Data")

ggplot(test, aes(x=num.Quality, y=num.predictions, color=followup)) +
  geom_jitter() +
   scale_colour_manual(values=c("grey50", "darkorange"))

```

The boxplot shows a general picture of the results of the model. There is a significant number of misclassified scans. Using a typical 0.5 rounding we get the confusion matrix: 
```{r}
# Top is true value, left is predicted
metrics <- confusionMatrix(factor(round(test$num.predictions)),
                           test$num.Quality, mode="everything", positive = "1")
metrics
```

## Followup Scans

XXX todo: add standard analysis and predicted outliers
XXX Consider manual g
